Understand why _standardize_user_data outputs lists...

Check computation of hessian diagonal.

Influence of batch size, learning rate and other learning parameters
(e.g. epochs)??? --> Measure "success rate" as a function of batch
size for several optimization methods...

Organize tests

Further speedup: avoid multiple data transfer between host and device.

Study influence of image pre-processing (intensity normalization...)

Make sure the Keras objective is maximum likelihood - seems quite
confusing.

How can we make sure that the stochastic gradient descent reached the
global optimum?

Is the droppout necessary? "Dropout consists in randomly setting a
fraction rate of input units to 0 at each update during training time,
which helps prevent overfitting."


BUGS

* The convolution does not work with GPU in float64 format. This is
very weird, it looks like the multiplication between kernel and src
data returns zero at fixed 2d locations.


DONE

* Implement sample weights in optimization

* Border effects

* Checks in Cython module: array dimension.

* Implement GPU max pooling

* Fix GPU convolution

* Non-contiguous buffer for multi-convolution (OpenCL implementation
requires contiguous arrays, not necessarily in C-order).

* GPU multi convolution

* Make OpenCL use FLOAT typedef rather than hard-coded float

* Pass compilation options nicely

* Pass CPU/GPU as argument to OpenCL.

* Move opencl source code into a subdirectory like a standard data directory.
