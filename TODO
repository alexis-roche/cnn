Implement sample weights in optimization

Understand why _standardize_user_data outputs lists...

Understand how Keras optimizer stores batch size, shuffle, and other similar options...

Organize tests

Further speedup: avoid multiple data transfer between host and device.

Study influence of image pre-processing (intensity normalization...)

Make sure the Keras objective is maximum likelihood - seems quite
confusing.

How can we make sure that the stochastic gradient descent reached the
global optimum?

Is the droppout necessary? "Dropout consists in randomly setting a
fraction rate of input units to 0 at each update during training time,
which helps prevent overfitting."

Influence of other learning parameters (e.g. epochs)???



BUGS

* The convolution does not work with GPU in float64 format. This is
very weird, it looks like the multiplication between kernel and src
data returns zero at fixed 2d locations.


DONE

* Border effects

* Checks in Cython module: array dimension.

* Implement GPU max pooling

* Fix GPU convolution

* Non-contiguous buffer for multi-convolution (OpenCL implementation
requires contiguous arrays, not necessarily in C-order).

* GPU multi convolution

* Make OpenCL use FLOAT typedef rather than hard-coded float

* Pass compilation options nicely

* Pass CPU/GPU as argument to OpenCL.

* Move opencl source code into a subdirectory like a standard data directory.
