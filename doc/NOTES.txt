Issues: 

* How should we choose the patch size? 

* Odd/even patch size? Should the center match an actual pixel?

* How do we make sure that examples are comprehensive? For instance,
  in the first running version, I observe that strong reflections on
  foil are often classified as food, presumably this is a consequence
  of not having many examples of that in the training dataset. 

* Should we have ambiguous examples?

* Should we balance classes in trainging (class_weight='auto' vs None)

* How to run the model in C/C++?

* Spatial regularization (Markov)

-----------------------------------------------------------------------

Efficiency of the patch-based approach. It seems that the community
has replaced patch-based CNNs with "fully convolutional nets" (FCN).

One thing that is worthwile noticing about CNNs is that they only
depend on the input image size IF they have fully connected layers.
However, the fully convolutional layers can be replaced with
convolutional layers of appropriate kernel sizes (depending on the
original input image size). 

This  seems to  be the  idea behind  FCN. There  is a  trivial way  to
transform a CNN into a FCN, the  advantage being that the FCN can take
up any  image size.  There seems  to be non-trivial  stuff in  the FCN
paper, but  let's first draw  some basic conclusions from  the trivial
part.

Et, donc, plutôt que d'appliquer un CNN indépendamment à toutes les
patchs d'une image indépendamment, il est beaucoup plus efficace
d'appliquer le FCN équivalent à l'image entière car les calculs sont
ainsi partagés à travers les patchs. C'est une remarque d'ordre
purement calculatoire, mais importante!

Si j'arrive à reverse-engineerer l'application d'un modèle par Keras,
ce qui ne devrait pas être bien difficile, je peux coder tout ça en C
indépendamment de toute librairie... et ça devrait être méga-efficace!

-----------------------------------------------------------------------
Feb 9.

Check padding 'same' or 'valid' in Keras. By default, Keras applies
'valid', which shrinks the input size. 'same' seems to be padding with
zeros. If 'valid', don't need to apply coordinate checks when applying
kernel mask, so may be substantially faster.

What is the concept behind Tensorflow sessions? Why converting a TF
tensor to a numpy array via the eval method requires to start a
fucking session, and WHY THE FUCK is the result fucking random?????
For now, use keras TF backend as a workaround.

OK, so it seems I have a running C implementation of multiple
convolution, just need to speed it up in the default case of 'valid'
padding. The ReLU/max pooling is still to be tested, but it's
difficult to screw up.

The main left difficulty will be to understand how the dense layers
are implemented and how they can be "convolutionalized"... Basically,
the convolutional layer ends up with a tensor of shape 4x4x64 in the
case of our CNN working on 50x50 image patches. So we have this
elongated 3D block of features fully connected to 64 units in this
instance, which amounts to 64 filters with kernel sizes (4, 4).

-----------------------------------------------------------------------
Feb 19.

Retour de vacances à Morillon, et comme souvent après une prise de
recul, le champ de vision s'élargit.

Ce qu'il faut implémenter, c'est une "convolution dilatée".

Il faut entrainer le CNN 'patch-based' avec padding 'valid' pour
éviter les effets de bord quand on implémente le fully convolutional
CNN équivalent. On a alors un effet de shift des coordonnées de bord
de patchs (+1 à chaque couche convolutionnelle) mais on s'en fout
peut-être un peu si on s'intéresse aux centres.

ATTENTION: le fully convolutional CNN doit, lui, maintenir les
dimensions spatiales constantes, donc le padding doit être de type
'same'... peu importe car les pixels concernés ne sont pas "valides"
au sens où ils ne sont pas au centre d'un patch pouvant être
classifié.

-----------------------------------------------------------------------
Feb 23.

Les premiers tests indiquent que le FCNN fait le job attendu avec un
temps de calcul environ 10 fois inférieur au code python basique (ne
partageant pas les calculs entre les patchs).

Il semble y avoir des effets de bord... Cela vient vraisemblablement
du fait que le zero-padding n'est pas approprié (s'il y a du signal au
bord de l'image, il va "baver" à l'extérieur par convolution mais cet
effet sera ignoré par les convolutions ultérieures). Une solution
simple serait d'étirer l'image de départ en la complétant
"physiquement" par des bords nuls suffisamment larges. Non, ce n'est
pas la même chose que faire du zero-padding à la volée (sauf à la
première convolution).

Ce sont clairement les convolutions qui prennent le plus de temps (par
rapport au max pooling). Il faut avoir à l'esprit que le coût
calculatoire d'une convolution dépend de:

- la taille du noyau: (sx, sy)
- le nombre de canaux en entrée: n0
- le nombre de canaux en sortie (nombre de filtres): n1

Grosso merdo, le temps de calcul est proportionnel à: sx*sy*n0*n1.

Ici, 

- pour la première convolution, on a: sx=sy=3, n0=3, n1=32.

- pour la convolution correspondant à la première couche "dense", on
  a: sx=sy=4, n0=64, n1=64.

Le rapport de temps de calcul sera donc de l'ordre de 76! Sur mon PC,
avec une compilation en O3, la première convolution prend environ 3/4
de seconde pour une image de taille 640x480, et la dernière prend
logiquement de l'ordre d'une minute.


-----------------------------------------------------------------------
DOUBLE vs FLOAT 

The code can work in float or double (depending on a typedef in
run_utils.h).

To probe the difference in encoding, I ran a convolution with kernel
size (4, 4, 64, 64) on a 640x480 image. The computation time was as
follows:

Double: 62.765 s
Float: 46.036 s

So, by using C-type float rather than double, we roughly reduce
computation time by 27%, in addition to reducing memory load by a
factor 2.

-----------------------------------------------------------------------
SHIFT

I observed that the FCN output was shifted by several pixels wrt the
result using the brute-force implementation of patched-based CNN
segmentation... why is it so?

This is normal. It can be seen that, for a CNN of the type considered
and with pool size 2, the top left corner of any patch is shifted by:

sum_{i=0}^{#conv_filters} 2^i = 2^{#conv_filters + 1} - 1
 
For instance, with 3 convolutional layers, the shift is of 2^4 - 1 =
15 pixels in both directions.

This means that the output corresponding to a patch top left corner
with coordinates x, y is found at (x+15, y+15) in the image produced
by the "equivalent" FCNN.

If now (x, y) are the patch center coordinates, the corresponding
output coordinates are (x-h+15, y-h+15) where h is half the patch
size. If we trained the CNN on patches of size 50x50, h=25, therefore
the shift is 15-25=-10, negative. This means that the pixel with true
coordinates (0, 0) is not found in the FCNN output as it would
correspond to coordinates (-10, -10).

To correct for that, we just need to translate the input image by 10
towards the bottom right before applying the FCNN.

If we do that using the same image size (hence truncating the input
image to the bottom right), we get the correct output up to some
clutter in the bottom right...  due to the truncation but also the
fact that the input should be padded with zeros. We can further
correct for this using a sufficiently large image size but this seems
too little a problem to deserve headaches.


-----------------------------------------------------------------------
DIMENSION FLOW

Assume kernel size = 3, pool size = 2

Convolutional layer: dim -> dim - 2
Pooling: dim -> dim / 2
Shift is sum_{i=0,...,#pool_steps} 2^i = 2^{#pool_steps + 1} - 1 

How does this generalize?

Kernel size. The image FOV is reduced to ensure that the kernel always
fully overlaps the image (with the 'valid' padding option).

* If the kernel size is odd, the dim is reduced by 2*((size-1)/2) =
size-1.

* If the kernel size is even, the kernel mask is asymetric, taking
  hl=(size-1)/2 pixels and hr=size/2 pixels on the right, hence we
  loose hl+hr = (size-1+size)/2 = (2*size-1)/2 = size-1, again!

--> the dimension reduction due to a convolution is always the kernel
    size minus one.

Pooling. We will barely deal with pooling sizes other than 2, but
let's do the excercise.

First, we have the same issue as for the kernel: the mask needs to
overlap the image completely, so the max filter (or any filter
associated with pooling) is applied to an image with dimensions
reduced by (pool_size-1).

Next, a subsampling operation is performed, which approximately
divides the dim by the pool size. We need to solve:

f * (d-1) < D => d < (D/f) + 1
f * d >= D => (D/f) <= d

=> d = ceil(D/f)

To sum up:

dim -> dim - pool_size + 1 -> ceil((dim - pool_size + 1) / pool_size)
= ceil((dim + 1) / pool_size) - 1

So the effect of convolution + pooling is to do:

dim -> ceil((dim + 1 - kernel_size + 1) / pool_size) - 1

= ceil((dim + 2 - kernel_size) / pool_size) - 1

Shift.

A la 1ere convolution, le coin se déplace de (kernel_size-1)/2
Au 1er pooling, il se déplace de (pool_size-1)/2
Notons s = (kernel_size-1)/2 + (kernel_size-1)/2

Puis on sous-échantillonne d'un facteur f.

A la 2e convolution, le facteur de dilatation vaut f, c-à-d qu'on vit
désormais dans un monde où la distance entre deux pixels voisins est
f. Donc, tout se passe comme avant sauf qu'on multiplie tout par
f... c'est simple.

Le shift vaudra donc: s + s * f

etc...

En notant n le nombre de couches convolutionnelles (suivies de
pooling), le shift total est:

s + sf + sf^2 + ...
= s (f^(n+1) - 1) / (f-1)

Dans notre CNN de base, nous avons s=1, f=2 et n=3.

-----------------------------------------------------------------------
March 2.

TODO:

-> figure out the resolution to use to achieve best performance/cost
   tradeoff. For instance, if we work at half the resolution, we can
   use patches of size 25x25 to train the CNN, and we will essentially
   divide the computation time by 4.

REFLEXIONS...

Je me pose toujours la question de savoir s'il faut prendre des patchs
de taille paire ou impaire.

Si c'est paire, le centre du patch ne coincide pas avec un
pixel. Est-ce que c'est emmerdant? Un peu, si le but est de classifier
chaque pixel en fonction du patch dont il est le centre. Mais c'est un
peu une question d'ordre philosophique car on pourrait dire qu'on
classifie chaque pixel en fonction du patch dont il est le pixel le
plus proche du centre - par valeurs inférieures ou supérieures, comme
on veut... Mais, justement, cet arbitraire n'est pas très souhaitable.

Cette question est à rapprocher à celle de l'entrainement utilisant ou
non des patchs ambigus. 

-----------------------------------------------------------------------
April 9.

Un autre effet de bords auquel on est confronté du fait de la
convolution 'valide' est que l'image de probabilité finale est
entourée d'une bande de pixels de valeur constante sans
signification. La bande est plus large à droite et en bas parce que la
troncature affecte préférentiellement les valeurs supérieures dans
notre implémentation (de façon cohérente avec Keras).

Par exemple, un pooling de 2 ne produit pas de troncature à gauche,
mais fait perdre une bande d'un pixel à droite et en bas.

Ceci s'ajoute au phénomène de déplacement discuté plus haut, lui aussi
dû à l'interpolation 'valide'.

Quand on applique un noyau de taille s avec une dilation de 1, on perd:
* à gauche: (s - 1) // 2
* à droite: s // 2

Avec un facteur de dilatation d, on perd:
* à gauche: d * ((s - 1) // 2)
* à droite: d * (s // 2)

Dans notre réseau favori, on perd:
* à gauche: 1 (conv), 0 (max pool), 2, 0, 4, 0, 8 (dense layer)
* à droite: 1, 1, 2, 2, 4, 4, 16

Ces pertes s'ajoutent à chaque itération, de sorte qu'on se fait
manger 15 pixels à gauche et 30 à droite. Ce sont des pixels pour
lesquels le FCNN n'est pas cohérent avec le CNN associé à cause d'un
champ de vue restreint. 

La perte liée aux convolutions (excepté la 1ère couche dense) est de
la forme:

p_conv = q + 2*q + 4*q + ... = q (1 + 2 + 4 + ... ) = q (2**n - 1)

avec q = (kernel_size-1)//2 à gauche et q=kernel_size//2 à droite, où
n est le nombre de couches convolutionnelles.

De même, pour les relouds Max/Poules, on perd:

p_remax = q (2**n - 1)

avec q = (pool_size-1)//2 à gauche et q=pool_size//2 à droite.

La formule générale est, en notant ks la kernel size, ps la pool size
et fs la kernel size de la convolution finale équivalente à la 1ere
couche dense:

* A gauche: p_left = [(ks-1)//2 + (ps-1)//2] (2**n - 1) + 2**n (fs-1)//2
* A droite: p_right = [ks//2 + ps//2] (2**n - 1) + 2**n fs//2

------------------------------------------

Apr 27.

Looks like, the more training converged, the more binary the CNN
output... Meaning that the network is more confident in its
predictions as training improves. Makes sense, doesn't it?

But could also suggest some kind of overfitting is happening. How can
we evaluate overfitting?

The answer, my friend, is running the EP. The answer is running the
EP.

Overfitting has to be reflected by high posterior variances on the
parameters. Low variances mean near certainty on parameter values. I
realize that not everyone might agree with that, but it's obvious to
me... Low variances mean the data at hand is enough for training, so
it can only mean underfitting. 

But what does EP provide us with? Parameter estimates and
variances. From there, we can evaluate the uncertainty in network
responses using some Monte Carlo simulation for a bunch of examples,
and pick those examples for which the uncertainty is largest. Then,
generate more of this type of examples and see if we improve things.

-------------------------------

July 9.

Reading notes from pyimagesearch

https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/
https://www.pyimagesearch.com/2017/09/18/real-time-object-detection-with-deep-learning-and-opencv/

Depthwise convolution? Purely spatial, channel-specific convolution,
resulting in one feature map per channel. Instead of the standard conv
+ max pool building block, we may use depthwise conv + max pool
followed by "1x1 convolution" + max pool. Advantage: for the same
number of scalar feature maps (same number of "filters"), we save a
lot of parameters. This trick is used for resource constrained
devices, hence the name "MobileNet". Drawback: sacrifice accuracy...

I need to understand the different types of object detection
approaches using deep learning. Object detection differs from image
classification in that we want to recognize any object present in an
image and locate it somehow (e.g., using a bounding box). R-CNN, Fast
R-CNN, Faster R-CNN, SSD...

Nice summary on object detection using CNN at:
https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9

"Most successful approaches to object detection are currently
extensions of image classification models" (Joyce Xu)

Old approach: R-CNN (Region-based CNN). Scan input image for possible
objects using a pre-processing method called "selective search", next
run a CNN on top of each region proposal (resizing image patches to a
standard size). Easy. As a cool add-on, use a linear regressor to
tighten the bounding box from the CNN features. R-CNN very intuitive
but very slow.

Selective search essentially relies on segmentation (looks like a good
old k-means clustering) to find "interesting" blobs... typically
resulting in about 2000 region proposals for an image. But at least we
won't run the CNN for each and every window location and scale.

Fast R-CNN. Girschick, 2015. Keep the selective search but apply a CNN
to the ENTIRE image to produce a feature map, from which a
"fixed-length feature vector" is extracted for each proposal region
(before the fully connected layer as far as I understand). So there is
an "RoI pooling layer" that acts as standard max pooling but can work
with any input shape (always producing the same output shape). To turn
a pre-trained CNN into a R-CNN, we replace the last max pooling layer
with a "compatible" RoI pooling layer.

In summary, Fast R-CNN takes as input a set of candidate regions in
addition to the input image, and outputs a probability mass over (K+1)
classes (including the "no object" class) for each candidate RoI (plus
additional bbox regressor parameters, but let's forget about that for
now). The main advantage over R-CNN is that computations are shared
across overlapping candidate RoIs.

Fast R-CNN still uses a selective search to find candidate RoIs, which
is slow, hence the motivation for a Faster R-CNN. Ren et al, 2015. In
short, Faster R-CNN = RPN + Fast R-CNN. RPN is some kind of CNN that
substitutes the selective search. So, what is a RPN?

RPN = Region proposal network. We have an initial CNN of which we
consider the last layer. We move a 3x3 sliding window across the
output of the last convolution layer and maps it to a lower dimension
vector (e.g., 256-d), which is used to predict whether various
pre-defined "anchor boxes" centered at the sliding window location
contain an object or not (it's really a binary decision: object vs
non-object as the goal is only to output candidate regions). In
practice, they use 9 anchor boxes corresponding to three different
aspect ratios (1:1, 2:1, 1:2) and three different scales.

At this point, we note that all these methods (R-CNN, Fast R-CNN,
Faster R-CNN) work in a two-step fashion: first select candidate
regions (using either selective search or RPN), second classify
them. The Single Shot Detector (SSD) method by Liu et al, 2016,
collapse both tasks into one, hence the term "single shot".

"The SSD approach is based on a feed-forward convolutional network
that produces a fixed-size collection of bounding boxes and scores for
the presence of object class instances in those boxes, followed by a
non-maximum suppression step to produce the final detections."

SSD expands on a "base network" suitable for image classification
"(truncated before any classification layers)". Convolutional feature
layers are added to the end of the truncated base network, which
decrease in size, producing multi-scale feature maps for detection.
Each such feature map is convolved by small 3x3 kernels to evaluate 3
default bounding boxes, producing a score for each category and 4
shape offset parameters. This bit is similar to Faster R-CNN.

At each location, for each box out of k, we compute c class scores and
the 4 offsets relative to the original default box shape, yielding
(c+4)k outputs per location in a m×n feature map.

At training, all the default boxes that have a significant overlap
with the ground truth (as measured by the Jacard index, or
"Intersection over Union") are considered as
"positives". Specifically, each ground truth box is matched with the
default box with largest IoU, and with all other default boxes with an
IoU > 0.5. The training loss is a combination of a "confidence" loss
(on the predicted class) and localization loss. 

------

Deep metric learning.

Again from this nice block, Adrian Rosebrock:

https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/

Common deep learning methods are classification methods. They take
some type of data as input and output a label. Deep metric learning is
concerned with methods that output a real-valued feature vector
instead.

Adrian mentions a deep metric learning approach for face recognition,
one that takes a face image patch as input and outputs a 128-d
vector. The idea behing the training of such a system is to use
triplets as data items - in this case, a triplet consists of two image
patches of the same guy and another patch from someone else. Each
triplet should result in tweaking the network weights so that the
vector outputs corresponding to the same guy are closer to each other
and farther from the output corresponding to the other guy.

Deep metric learning extracts a meaningful representation of a face
image, in this case 128-d, which can be fed into a standard
classification technique, e.g., a k-NN classifier for actual face
recognition.

A general remark is that a standard feed-forward network for
classification also extracts real-valued feature vectors before
performing a classification task in the last layer. Can this feature
extraction be considered as a case of "deep metric learning"? Or, in
other words, are these features useful to other tasks than the
classification at hand?  This is kind of the question underlying
transfer learning.

One advantage of metric learning is that it seems to be a more direct
approach: try to represent a complex object by a lower-dimensional
"signature" in such a way that classification is made easy.

On the other hand, transfer learning relies on much smaller training
datasets, making it much more affordable in practice.

Another approach is to design and train a network to decide whether
two input images represent the same person or not. This is not metric
learning per se since the network output is a binary variable (as
opposed to a dense representation) but the network similarly learns
a representation that needs to be person-specific. 


-----------

One thing that comes to my mind is that SSD looks like a brute-force
approach to solve the object recognition problem.

What about a multi-scale version of FCNN? Whatever it means... I need
to think into this.

------------

July 24/25.

What is "softmax cross entropy with logits" (tensorflow expression)?

Let us recall that the cross-entropy between two distributions p and q
is defined by:

H(p, q) = - sum_z p(z) log q(z) = - E_p[log q(Z)]

Now consider the log-likelihood:

L = sum_i log q(xi, yi) = sum_i [log q(yi|xi) + log q(xi)]

where xi is i-th example input and yi the corresponding output and
q(x, y) is the joint distribution implemented by the network. In
practice, the marginal q(x) is not modeled so it can be anything we
want but we simply don't care about this term.

L is fundamentally the same thing as the cross-entropy:

L = -n H(p, q)

where p(z) represents the empirical distribution of z=(x,y) in the
training dataset and q is the model distribution. 

If we drop the marginal term q(x), we see that L is just a negated and
normalized version of the CONDITIONAL cross-entropy, say:

L = -n H_q(Y|X)

where the conditional cross-entropy reads:

H_q(Y|X) = - sum_x,y p(x,y) log q(y|x)

In the case p=q, the conditional cross-entropy is simply the
well-known conditional entropy.

In conclusion, when using anything called cross-entropy as a training
score, one simply aims to maximize likelihood.

Note that the cross-entropy decomposes as the sum of the entropy and
the Kullback-Leibler divergence:

H(p, q) = H(p) + D(p||q)

This is also true in the conditional case.

In practice, `p` is only an empirical estimate of the `true` joint
input x output distribution, but let us assume that the training
dataset is so big that the approximation is accurate.

H(p) would vanish (in any task where the output takes on discrete
values, such as classification) if there exists a way to complete the
task with absolute certainty, i.e. if the label is a deterministic
function of the input data. For instance, if we tackle a vision task
that can be accomplished faultlessly by the human eye, we can assume
that H(p)=0. In this case, the MINIMUM cross-entropy simply reflects
the lack of modeling power of the network, which is a purely EXTRINSIC
source of uncertainty.

More generally, if the task cannot be made 100% reliable by any
device, then cross-entropy reflects a combination of intrinsic and
extrinsic sources of uncertainty. The entropy can be thought of as
some sort of baseline uncertainty measure (you can't get lower than
that unless you measure the label).

From a pragmatic viewpoint, it does not matter where the uncertainty
comes from. The minimum cross-entropy tells us "how unreliable" our
predictive model is - and, don't forget, it's only an empirical
estimate.

The entropy H(p) is a lower bound on the cross-entropy achieved by the
model. It might not be easy to estimate in practice with a dense input
variable, even using big data. However, as already pointed out, we
might assume that H(p) is zero or close to it.

What about the cross-entropy of a random classifier? A random
classifier is one for which q(y|x)=q(y) is independent from x. It is
then obvious that the conditional cross-entropy boils down to the
marginal cross-entropy:

H(p, q) = - sum_y p(y) log q(y) >= H(p)

Hence, the "best" random classifier is the one that matches the
marginal distribution of classes. If that distribution is uniform
(which should be roughly the case if examples are balanced using
appropriate weights), then the entropy is log(K), where K is the
number of classes. In classification, this limit is about 0.69 (nats).

Seems that TensorFlow and Keras use natural or base-2 log for entropy
computation, meaning that they return entropy values in nats as
opposed to bits.

What does it mean? It means that I expect my binary classifier to
achieve a cross-entropy somehwere between 0 and 0.69 nats at
training. Larger than 0.69 is possible but totally useless as it would
mean that the classifier is worse than a trivial classifier, namely
the "best" random classifier.

Obviously, the closer to zero, the better. How close to zero we need
to get to be confident on our classifier is a tricky question, but
this simple reasoning on cross-entropy gives a rule of thumb to make
sense of training logs.

We shall keep in mind that cross-entropy is estimated on
mini-batches. However, if values are fairly consistent across
mini-batches, they should give a reasonable guess of the actual
cross-entropy.

Now, a more tricky question: can we have a perfect classification rate
with a large cross-entropy? In theory, this is possible... yes... But
if the model is trained to maximize the model likelihood, one should
not be concerned about the classification rate. The fundamental goal
is to approximate the `true' conditional distribution, that is, try
and capture the intrinsic uncertainty on predictions as much as
possible.


